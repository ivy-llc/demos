{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsW9YucKLwmo",
        "outputId": "489378ea-a054-468b-bf11-7011924398b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q ivy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF-rwYGRCF9j"
      },
      "outputs": [],
      "source": [
        "import ivy\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2LNaAXxDWc-",
        "outputId": "ba1bf4f2-81c2-45ea-baa5-59c9cf5c2a43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ivy/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Ivy would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing ivy.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# create torch lstm layer\n",
        "torch_lstm = torch.nn.LSTM(2, 2, 1)\n",
        "\n",
        "# transpile lstm layer to tensorflow\n",
        "x = torch.rand((5, 2, 2))\n",
        "tf_lstm = ivy.transpile(torch_lstm, source=\"torch\", to=\"tensorflow\", args=(x,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPLdIj2CD7pA",
        "outputId": "f43e3b59-2ca3-49f6-f6d0-0a5e8d14dd73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get output of original torch lstm layer\n",
        "x = torch.rand((20, 32, 2))\n",
        "original_output = torch_lstm(x)\n",
        "\n",
        "# get output of transpiled tf lstm layer with the same input\n",
        "x = tf.constant(x.cpu().numpy())\n",
        "transpiled_output = tf_lstm(x)\n",
        "\n",
        "# verify the outputs are the same (with some tolerance)\n",
        "np.allclose(original_output[0].detach().cpu(), transpiled_output[0].numpy(), atol=1e-7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22XU0XD277Y4",
        "outputId": "91c35ac9-b282-46a2-b33a-963c9dbc1cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transpiled tf time is 4.480074623755541x slower than torch's lstm\n",
            "original tf lstm time is 2.362692848996253x slower than torch's lstm\n"
          ]
        }
      ],
      "source": [
        "# run some benchmarks\n",
        "from time import perf_counter\n",
        "\n",
        "x = torch.rand((20, 32, 2))\n",
        "N_RUNS = 1000\n",
        "\n",
        "# time the original torch lstm\n",
        "s = perf_counter()\n",
        "for _ in range(N_RUNS):\n",
        "  torch_lstm(x)\n",
        "original_torch_time = perf_counter() - s\n",
        "\n",
        "# compile transpiled tf lstm\n",
        "x = tf.constant(x.cpu().numpy())\n",
        "tf_lstm = tf.autograph.experimental.do_not_convert(tf_lstm)\n",
        "compiled_tf_lstm = tf.function(tf_lstm)\n",
        "compiled_tf_lstm(x)\n",
        "\n",
        "# time the transpiled tf lstm\n",
        "s = perf_counter()\n",
        "for _ in range(N_RUNS):\n",
        "  compiled_tf_lstm(x)\n",
        "transpiled_tf_time = perf_counter() - s\n",
        "\n",
        "# time tf's own lstm layer (also compiled) for comparison\n",
        "original_tf_lstm = tf.keras.layers.LSTM(2, time_major=True, return_sequences=True)\n",
        "original_tf_lstm = tf.function(original_tf_lstm)\n",
        "original_tf_lstm(x)\n",
        "\n",
        "s = perf_counter()\n",
        "for _ in range(N_RUNS):\n",
        "  original_tf_lstm(x)\n",
        "original_tf_time = perf_counter() - s\n",
        "\n",
        "# as we can see, the transpiled tf lstm has comparable performance to tf's own lstm layer\n",
        "print(f'transpiled tf time is {transpiled_tf_time / original_torch_time}x slower than torch\\'s lstm')\n",
        "print(f'original tf lstm time is {original_tf_time / original_torch_time}x slower than torch\\'s lstm')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
